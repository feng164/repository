# 机器学习

## 监督学习与无监督学习

### 1. 监督学习（Supervised Learning）

**（1）定义**

- 监督学习是一种机器学习方法，其训练数据包含**<u>输入特征和对应的输出标签</u>**。模型通过学习输入特征和输出标签之间的映射关系，来预测新的、未见过的数据的输出标签。
- 例如，在图像分类任务中，训练数据包含图像（输入特征）和对应的类别标签（如“猫”“狗”等）。模型通过学习这些图像和标签之间的关系，来预测新图像的类别。

**（2）常见算法**

- **回归算法**：用于预测连续值输出。例如，`线性回归（Linear Regression)` 用于预测房价、股票价格等连续数值。
- **分类算法**：用于预测离散值输出。例如，`逻辑回归（Logistic Regression）`、`支持向量机（SVM）`、`决策树（Decision Tree）`、`随机森林（Random Forest）`、`K近邻算法（KNN）`等。这些算法广泛应用于图像分类、文本分类、医学诊断等领域。

**（3）应用场景**

- **预测任务**：如股票价格预测、房价预测、销售量预测等。
- **分类任务**：如垃圾邮件检测（将邮件分为“垃圾邮件”和“非垃圾邮件”）、疾病诊断（将患者分为“患病”和“未患病”）、图像分类（将图像分为不同的类别）等。
- **推荐系统**：如将用户的行为（如点击、购买）作为标签，预测用户对新物品的兴趣。

### 2. 无监督学习（Unsupervised Learning）

**（1）定义**

- 无监督学习是一种机器学习方法，其训练数据**<u>只包含输入特征，而没有对应的输出标签</u>**。模型通过挖掘数据中的内在结构和模式，来发现数据的分布*规律*或进行数据的*聚类*。
- 例如，在客户细分任务中，企业只有客户的购买行为数据，而没有预先定义的客户类别。无监督学习可以通过分析这些数据，将客户划分为不同的群体。

**（2）常见算法**

- **聚类算法**：用于将数据划分为不同的簇，使同一簇内的数据相似度较高，而不同簇之间的数据相似度较低。常见的聚类算法包括：
  - `K均值聚类（K-Means）`：通过计算数据点与簇中心的距离，将数据点分配到最近的簇。
  - `层次聚类（Hierarchical Clustering）`：通过构建层次化的簇结构，可以生成不同粒度的聚类结果。
  - `DBSCAN（基于密度的空间聚类）`：通过分析数据点的密度，将高密度区域划分为簇，同时识别出噪声点。
- **降维算法**：用于将高维数据映射到低维空间，以便更好地理解和可视化数据。常见的降维算法包括：
  - `主成分分析（PCA）`：通过线性变换，将数据投影到主成分方向上，去除数据中的冗余信息。
  - `t-SNE（t分布随机邻域嵌入）`：适用于高维数据的可视化，能够将数据映射到二维或三维空间，同时保持数据的局部结构。

**（3）应用场景**

- **数据探索与分析**：如，分析用户行为数据，发现用户的不同行为模式。
- **聚类分析**：如客户细分（将客户划分为不同的群体，以便进行精准营销）、图像分割（将图像划分为不同的区域）、文档聚类（将文本划分为不同的主题）等。
- **降维与特征提取**：在高维数据处理中，通过降维算法可以降低数据的维度，减少计算复杂度，同时提取出更有代表性的特征。

### 3. 区别

| 特点     | 监督学习                                             | 无监督学习                                                   |
| -------- | ---------------------------------------------------- | ------------------------------------------------------------ |
| 数据需求 | 需要标注数据（输入特征 + 输出标签）                  | 只需要输入特征，不需要输出标签                               |
| 目标     | 学习输入特征和输出标签之间的映射关系，用于预测或分类 | 发现数据中的内在结构和模式，如聚类、降维等                   |
| 常见算法 | 回归算法（线性回归等）、分类算法（SVM、决策树等）    | 聚类算法（K-Means、DBSCAN等）、降维算法（PCA、t-SNE等）      |
| 应用场景 | 预测任务（房价预测等）、分类任务（图像分类等）       | 数据探索（发现潜在模式）、聚类分析（客户细分等）、降维与特征提取 |
| 优点     | 目标明确，性能可评估，适用于多种任务                 | 不需要标注数据，可发现隐藏结构                               |
| 缺点     | 需要大量标注数据，标注成本高                         | 输出解释性弱，效果不稳定，缺乏明确的评估指标                 |

## 线性回归模型(Linear Regression)

### 1.线性回归数学公式

$f_w,_b(x)=w_0*x_0+w_1*x_1+...+w_n*x_n+b(wx+b)$

- f(x)是目标输出
- x~1~,x~2~,…x~n~是输入特征
- w~0~,w~1~,…w~n~是参数
- b是截距参数

### 2.代价函数(Cost Function)

$J(w,b)=\frac{1}{2m}\sum_{i=1}^{m}(f(x_i)-y^i)^2$

- m为训练集的数量
- y(i)为准确值,f(x~i~)为预测值
- 2m更好求导去求min( J(w,b) )

- w,b 两个参数使得J图像是三维的
![image-20250211220009100](https://typora10213.oss-cn-guangzhou.aliyuncs.com/undefinedimage-20250211220009100.png)

![image-20250211220444181](https://typora10213.oss-cn-guangzhou.aliyuncs.com/undefinedimage-20250211220444181.png)

### 3.梯度下降(Gradient Descent)

- 梯度下降是一种常用的优化算法，通过迭代更新参数 ，使其沿着损失函数的低数方向逐步逼近最优解。

- 设参数为*θ*,则: $θ_j=θ_j-α\frac{d}{dθ_j}J(θ_0,θ_1,...,θ_n)$

  - 其中 “=”为赋值而非等号

    - ```python
      temp_w=w-$α\frac{d}{dw}J(w,b)$
      temp_b=b-$α\frac{d}{db}J(w,b)$
      #同时更新
      w=temp_w
      b=temp_b
      ```

  - `α`为学习率(Learning Rate),控制每次迭代的步长.控制α的大小以防达到局部最小值而是达到全局最小值

  - `α`后的为损失函数对参数θ~j~的偏导数
  
    - $\frac{d}{dw}J(w,b)=\frac{1}{m}\sum_{i=1}^{m}(f_w,_b(x_i)-y_i)x_i$ 
  
    - $\frac{d}{db}J(w,b)=\frac{1}{m}\sum_{i=1}^{m}(f_w,_b(x_i)-y_i)$

