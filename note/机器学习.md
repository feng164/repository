# 机器学习

## 监督学习与无监督学习



### 1. 监督学习（Supervised Learning）

#### **（1）定义**

- #####  监督学习是一种机器学习方法，其训练数据包含**<u>输入特征和对应的输出标签</u>**。模型通过学习输入特征和输出标签之间的映射关系，来预测新的、未见过的数据的输出标签。

- ##### 例如，在图像分类任务中，训练数据包含图像（输入特征）和对应的类别标签（如“猫”“狗”等）。模型通过学习这些图像和标签之间的关系，来预测新图像的类别。

#### **（2）常见算法**

- #####  **回归算法**：用于预测连续值输出。例如，`线性回归（Linear Regression)` 用于预测房价、股票价格等连续数值。

- #####  **分类算法**：用于预测离散值输出。例如，`逻辑回归（Logistic Regression）`、`支持向量机（SVM）`、`决策树（Decision Tree）`、`随机森林（Random Forest）`、`K近邻算法（KNN）`等。这些算法广泛应用于图像分类、文本分类、医学诊断等领域。

#### **（3）应用场景**

- #####  **预测任务**：如股票价格预测、房价预测、销售量预测等。

- #####  **分类任务**：如垃圾邮件检测（将邮件分为“垃圾邮件”和“非垃圾邮件”）、疾病诊断（将患者分为“患病”和“未患病”）、图像分类（将图像分为不同的类别）等。

- #####  **推荐系统**：如将用户的行为（如点击、购买）作为标签，预测用户对新物品的兴趣。

### 2. 无监督学习（Unsupervised Learning）

#### **（1）定义**

- #####  无监督学习是一种机器学习方法，其训练数据**<u>只包含输入特征，而没有对应的输出标签</u>**。模型通过挖掘数据中的内在结构和模式，来发现数据的分布*规律*或进行数据的*聚类*。

- #####  例如，在客户细分任务中，企业只有客户的购买行为数据，而没有预先定义的客户类别。无监督学习可以通过分析这些数据，将客户划分为不同的群体。

#### **（2）常见算法**

- #####  **聚类算法**：用于将数据划分为不同的簇，使同一簇内的数据相似度较高，而不同簇之间的数据相似度较低。常见的聚类算法包括：

- #####  `K均值聚类（K-Means）`：通过计算数据点与簇中心的距离，将数据点分配到最近的簇。

- #####  `层次聚类（Hierarchical Clustering）`：通过构建层次化的簇结构，可以生成不同粒度的聚类结果。

- #####  `DBSCAN（基于密度的空间聚类）`：通过分析数据点的密度，将高密度区域划分为簇，同时识别出噪声点。

- #####  **降维算法**：用于将高维数据映射到低维空间，以便更好地理解和可视化数据。常见的降维算法包括：

- #####  `主成分分析（PCA）`：通过线性变换，将数据投影到主成分方向上，去除数据中的冗余信息。

- #####  `t-SNE（t分布随机邻域嵌入）`：适用于高维数据的可视化，能够将数据映射到二维或三维空间，同时保持数据的局部结构。

#### **（3）应用场景**

- #####  **数据探索与分析**：如，分析用户行为数据，发现用户的不同行为模式。

- #####  **聚类分析**：如客户细分（将客户划分为不同的群体，以便进行精准营销）、图像分割（将图像划分为不同的区域）、文档聚类（将文本划分为不同的主题）等。

- #####  **降维与特征提取**：在高维数据处理中，通过降维算法可以降低数据的维度，减少计算复杂度，同时提取出更有代表性的特征。

### 3. 区别

| 特点     | 监督学习                                             | 无监督学习                                                   |
| -------- | ---------------------------------------------------- | ------------------------------------------------------------ |
| 数据需求 | 需要标注数据（输入特征 + 输出标签）                  | 只需要输入特征，不需要输出标签                               |
| 目标     | 学习输入特征和输出标签之间的映射关系，用于预测或分类 | 发现数据中的内在结构和模式，如聚类、降维等                   |
| 常见算法 | 回归算法（线性回归等）、分类算法（SVM、决策树等）    | 聚类算法（K-Means、DBSCAN等）、降维算法（PCA、t-SNE等）      |
| 应用场景 | 预测任务（房价预测等）、分类任务（图像分类等）       | 数据探索（发现潜在模式）、聚类分析（客户细分等）、降维与特征提取 |
| 优点     | 目标明确，性能可评估，适用于多种任务                 | 不需要标注数据，可发现隐藏结构                               |
| 缺点     | 需要大量标注数据，标注成本高                         | 输出解释性弱，效果不稳定，缺乏明确的评估指标                 |

## 线性回归模型(Linear Regression)

### 1.线性回归数学公式

#### $f_w,_b(x)=w_0*x_0+w_1*x_1+...+w_n*x_n+b(wx+b)$

- f(x)是目标输出
- x~1~,x~2~,…x~n~是输入特征
- w~0~,w~1~,…w~n~是参数
- b是截距参数

### 2.代价函数(Cost Function)

#### $J(w,b)=\frac{1}{2m}\sum_{i=1}^{m}(f(x_i)-y^i)^2$

- m为训练集的数量
- y(i)为准确值,$f(x_i)$为预测值
- 2m更好求导去求min($J(w,b)$)

- w,b 两个参数使得J图像是三维的
  ![image-20250211220009100](https://typora10213.oss-cn-guangzhou.aliyuncs.com/undefinedimage-20250211220009100.png)

![image-20250211220444181](https://typora10213.oss-cn-guangzhou.aliyuncs.com/undefinedimage-20250211220444181.png)

### 3.梯度下降(Gradient Descent)

 梯度下降是一种常用的优化算法，通过迭代更新参数 ，使其沿着损失函数的低数方向逐步逼近最优解。

 设参数为*θ*,则:  $θ_j=θ_j-α\frac{d}{dθ_j}J(θ_0,θ_1,...,θ_n)$

-  其中 “=”为赋值而非等号

```python
  temp_w=w-$α\frac{d}{dw}J(w,b)$
  temp_b=b-$α\frac{d}{db}J(w,b)$
  #同时更新
  w=temp_w
  b=temp_b
```

- `α`为学习率(Learning Rate),控制每次迭代的步长.控制α的大小以防达到局部最小值而是达到全局最小值

- `α`后的为损失函数对参数θ~j~的偏导数

  - ####  $\frac{d}{dw}J(w,b)=\frac{1}{m}\sum_{i=1}^{m}(f_w,_b(x_i)-y_i)x_i$

  - ####  $\frac{d}{db}J(w,b)=\frac{1}{m}\sum_{i=1}^{m}(f_w,_b(x_i)-y_i)$

## 逻辑回归(Logistic Regression)

### 1.定义与原理:

-   **定义**：逻辑回归是一种用于处理二分类问题的有监督学习算法，它通过建立一个逻辑函数（Logistic Function）来预测事件发生的概率，进而根据概率进行分类 , 目标是通过输入特征（自变量）来预测一个二元因变量（如0或1、是或否、正类或负类）

-   **原理**：逻辑回归的核心是逻辑函数，也称为 Sigmoid 函数，其表达式为$g(z)=\frac{1}{1+e^-z}$，用于将线性回归的输出映射到(0, 1)区间，从而表示为概率值。其中 , $z=\vec{w}·\vec{x}+b$  . 该函数可以将线性组合的值映射到到之间的概率值，通常将概率值大于等于0.5为正类，小于的为负类。
    -   **决策边界(Decision Boundary)** : 当z=0时, 该线性回归的图像是一条直线, 为决策边界 . 一边令$z>=0,g(z)>=0.5,\hat{y}=1$, 另一边令$z<0,g(z)<0.5,\hat{y}=0$


### 2.模型训练

- **损失函数**：逻辑回归常用的损失函数是**交叉熵损失**（Cross-Entropy Loss）[对数损失函数] : 对于单个样本$(x_i,y_i)$，其损失函数为$L(y_i,\hat{y_i})=-[y_ilog(\hat{y_i})+(1-y_i)log(1-\hat{y_i})]$，其中$y_i$是真实标签，是$y^{hat}_i$模型预测的概率 ; 对于整个训练数据集，损失函数是所有样本损失的平均值$L=-\frac{1}{N}\sum^N_{i=1}[y_ilog(\hat{y{i}})+(1-y_i)log(1-\hat{y_i})]$。

- **梯度下降** : 

  - $w=w-α\frac{dL}{dw}$

  - $b=b-α\frac{dL}{db}$

## 过拟合

### 1.过拟合问题

#### **(1)定义**:

过拟合是指模型在训练数据上表现得非常好，能够很好地拟合训练数据中的噪声和细节，但在新的、未见过的数据（测试数据）上表现不佳，泛化能力差。

#### (2)原因

1. **模型复杂度高**：模型具有过多的参数或过于复杂的结构，使得它能够记住训练数据中的所有细节，包括噪声，而不是学习到数据中的真实模式。
2. **数据量不足**：训练数据量过少，模型没有足够的数据来学习到普遍的规律，容易过度依赖训练数据中的特定样本，导致过拟合。

### 2.解决过拟合的方法

1. **增加数据量**：收集更多的数据进行训练，使模型能够学习到更广泛的模式，减少对特定数据的依赖。
2. **数据增强**：通过对现有数据进行变换，如旋转、翻转、缩放等，生成更多的训练数据，增加数据的多样性。
3. **降低模型复杂度**：选择更简单的模型结构，或者减少模型的参数数量。例如，在神经网络中，可以减少层数或神经元数量。
4. **正则化**：在模型的损失函数中添加惩罚项，限制模型参数的大小，防止模型过度拟合。

### 3.正则化

#### (1)定义

正则化是一种通过在模型的损失函数中添加额外的惩罚项来限制模型复杂度的方法，其目的是防止模型过拟合，提高模型的泛化能力。

#### (2)作用

1. **控制模型复杂度**：通过惩罚较大的参数值，使模型倾向于选择更简单的解，避免模型过于复杂而过度拟合训练数据。
2. **减少参数数量**：有些正则化方法可以使一些参数变为零，从而实现特征选择的效果，减少模型的复杂度。

#### 常见的正则化方法

1. **L1正则化**：也称为Lasso正则化，在损失函数中添加参数的绝对值之和作为惩罚项。L1正则化可以产生稀疏的参数解，即一些参数会被压缩为零，从而实现特征选择的功能。
2. **L2正则化**：也称为岭回归正则化，在损失函数中添加参数的平方和作为惩罚项。L2正则化可以使参数值变小，但不会使参数变为零，它可以减少参数的波动，使模型更加稳定。

### 4.线性回归的正则化

- **L1正则化**：

  $J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})^2 + \frac{\lambda}{m} \sum_{j=1}^{n} |\theta_j|$

- **L2正则化**：

  $J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})^2 + \frac{\lambda}{2m} \sum_{j=1}^{n} \theta_j^2$

其中，m 是训练样本数量，λ 是正则化参数，用于控制正则化的强度。为了使$J(\theta)$足够小 ,  则将$\lambda$设置为合适大小使得$\theta_j$足够小

### 5.逻辑回归的正则化

- **L1正则化**：

  $J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log(h_{\theta}(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_{\theta}(x^{(i)})) \right] + \frac{\lambda}{m}\sum_{j=1}^{n} |\theta_j|$

- **L2正则化**：

  $J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log(h_{\theta}(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_{\theta}(x^{(i)})) \right] + \frac{\lambda}{2m} \sum_{j=1}^{n} \theta_j^2$

